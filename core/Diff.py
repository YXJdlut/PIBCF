import functools
import logging
import torch
import torch.nn as nn
from torch.nn import init
from torch.nn import modules
logger = logging.getLogger('base')
####################
# initialize
####################
import torch
import torch.nn as nn



logger = logging.getLogger('base')


class DiffusionNet(nn.Module):
    """
    åŒè¾“å…¥ï¼ˆvis, irï¼‰ï¼Œè¾“å‡ºä¸€ä¸ªèåˆå›¾åƒï¼ˆSRï¼‰ã€‚
    é€šè¿‡ opt æ„å»ºç½‘ç»œï¼Œå¦‚æœåˆå§‹åŒ–æ—¶æ²¡ä¼  optï¼Œ
    å°±ä¼šä½¿ç”¨ set_default_opt(opt) è®¾ç½®çš„å…¨å±€é»˜è®¤é…ç½®ã€‚
    """
    _default_opt = None   # ç±»å˜é‡ï¼Œå…¨å±€å…±äº« opt

    @classmethod
    def set_default_opt(cls, opt):
        """
        è®¾ç½®å…¨å±€é»˜è®¤ optï¼Œåœ¨è®­ç»ƒå…¥å£è°ƒç”¨ä¸€æ¬¡å³å¯ã€‚
        """
        cls._default_opt = opt

    def __init__(self, opt=None, device="cuda"):
        super(DiffusionNet, self).__init__()
        self.device = device
        # å¦‚æœæ²¡ä¼  optï¼Œå°±ç”¨å…¨å±€é»˜è®¤
        if opt is None:
            if DiffusionNet._default_opt is None:
                raise RuntimeError("âŒ è¯·å…ˆè°ƒç”¨ DiffusionNet.set_default_opt(opt)")
            opt = DiffusionNet._default_opt

        # è°ƒç”¨æœ¬æ–‡ä»¶é‡Œçš„ define_G æ„å»ºç½‘ç»œ
        self.netG = define_G(opt).to(device)
        self.schedule_phase = None

        # ğŸ‘‡ è¿™é‡Œè¡¥å……åˆå§‹åŒ–å™ªå£°è°ƒåº¦ï¼ˆå…³é”®ï¼‰
        self.netG.set_new_noise_schedule(
            opt['model']['beta_schedule']['train'], device
        )

    def forward(self, vis, ir, continous=False, mode="default"):
        """
        è¾“å…¥: vis, ir (tensor)
        è¾“å‡º: èåˆå›¾åƒ SR
        """
        data = {"vis": vis, "ir": ir}

        if mode == "default":
            SR, _, _ = self.netG.super_resolution(data, continous)
        elif mode == "ddim":
            SR, _, _ = self.netG.super_resolution_ddim(data, continous)
        else:
            raise NotImplementedError(f"Unknown mode: {mode}")
        if SR.dim() == 3:
            SR = SR.unsqueeze(0)

        return SR, None, None, None, None




def weights_init_normal(m, std=0.02):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.normal_(m.weight.data, 0.0, std)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        init.normal_(m.weight.data, 0.0, std)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm2d') != -1:
        init.normal_(m.weight.data, 1.0, std)  # BN also uses norm
        init.constant_(m.bias.data, 0.0)


def weights_init_kaiming(m, scale=1):
    classname = m.__class__.__name__
    if classname.find('Conv2d') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
        m.weight.data *= scale
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')
        m.weight.data *= scale
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm2d') != -1:
        init.constant_(m.weight.data, 1.0)
        init.constant_(m.bias.data, 0.0)


def weights_init_orthogonal(m):
    classname = m.__class__.__name__
    if classname.find('Conv') != -1:
        init.orthogonal_(m.weight.data, gain=1)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('Linear') != -1:
        init.orthogonal_(m.weight.data, gain=1)
        if m.bias is not None:
            m.bias.data.zero_()
    elif classname.find('BatchNorm2d') != -1:
        init.constant_(m.weight.data, 1.0)
        init.constant_(m.bias.data, 0.0)


def init_weights(net, init_type='kaiming', scale=1, std=0.02):
    # scale for 'kaiming', std for 'normal'.
    logger.info('Initialization method [{:s}]'.format(init_type))
    if init_type == 'normal':
        weights_init_normal_ = functools.partial(weights_init_normal, std=std)
        net.apply(weights_init_normal_)
    elif init_type == 'kaiming':
        weights_init_kaiming_ = functools.partial(
            weights_init_kaiming, scale=scale)
        net.apply(weights_init_kaiming_)
    elif init_type == 'orthogonal':
        net.apply(weights_init_orthogonal)
    else:
        raise NotImplementedError(
            'initialization method [{:s}] not implemented'.format(init_type))

# Generator
def define_G(opt):
    model_opt = opt['model']
    if model_opt['which_model_G'] == 'ddpm':
        from .ddpm_modules import diffusion, unet
    elif model_opt['which_model_G'] == 'sr3':
        from .sr3_modules import diffusion, unet
    elif model_opt['which_model_G'] == 'diffif':
        from .diffif_modules import diffusion, unet, finetune_arch

    if ('norm_groups' not in model_opt['unet_denoising']) or model_opt['unet_denoising']['norm_groups'] is None:
        model_opt['unet_denoising']['norm_groups'] = 24

    model = unet.UNet(
        in_channel=model_opt['unet_denoising']['in_channel'],
        out_channel=model_opt['unet_denoising']['out_channel'],
        norm_groups=model_opt['unet_denoising']['norm_groups'],
        inner_channel=model_opt['unet_denoising']['inner_channel'],
        channel_mults=model_opt['unet_denoising']['channel_multiplier'],
        attn_res=model_opt['unet_denoising']['attn_res'],
        res_blocks=model_opt['unet_denoising']['res_blocks'],
        dropout=model_opt['unet_denoising']['dropout']
    )

    model_refinement_fn = finetune_arch.Restormer_fn(
        in_channel=model_opt['unet_refine']['in_channel'],
        out_channel=model_opt['unet_refine']['out_channel']
    )

    netG = diffusion.GaussianDiffusion(
        denoise_fn=model,
        refinement_fn=model_refinement_fn,
        image_size=model_opt['diffusion'].get('image_size', 256),
        channels=model_opt['diffusion']['channels'],
        loss_type='l1',  # opt: l1/l2
        conditional=model_opt['diffusion']['conditional'],
        schedule_opt=model_opt['beta_schedule']['train']
    )

    if opt['phase'] == 'train':
        init_weights(netG, init_type='orthogonal')

    # âŒ åˆ é™¤ GPU ç›¸å…³é€»è¾‘ï¼Œä¸å¤„ç† gpu_ids/distributed
    return netG

